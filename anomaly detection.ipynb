{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df5b24c-87be-4ad0-9a8c-6a69e7825b93",
   "metadata": {},
   "source": [
    "Anomaly detection, also known as outlier detection, is a technique in machine learning and data mining that involves identifying patterns or instances in a dataset that do not conform to the expected behavior. Anomalies are data points that deviate significantly from the majority of the data and are often considered rare or unusual occurrences. The primary purpose of anomaly detection is to discover and flag instances that are not in line with the norm, potentially indicating interesting and unexpected events or issues.\n",
    "\n",
    "**Key Concepts and Objectives of Anomaly Detection:**\n",
    "\n",
    "1. **Identifying Unusual Patterns:**\n",
    "   - Anomaly detection aims to identify instances that exhibit patterns significantly different from the majority of the data. These patterns could be in the form of outliers, novelties, or abnormalities.\n",
    "\n",
    "2. **Detecting Unexpected Events:**\n",
    "   - The primary purpose is to detect events or data points that are unexpected or rare. These unexpected events could be indicative of errors, fraud, faults, or other issues, depending on the application domain.\n",
    "\n",
    "3. **Maintaining Data Quality:**\n",
    "   - Anomaly detection is often used to ensure data quality and integrity by identifying and addressing data points that may be erroneous, noisy, or represent data entry mistakes.\n",
    "\n",
    "4. **Improving Security:**\n",
    "   - In cybersecurity, anomaly detection is employed to identify unusual patterns or behaviors in network traffic, potentially indicating malicious activities or security breaches.\n",
    "\n",
    "5. **Preventing Fraud:**\n",
    "   - Anomaly detection is commonly used in fraud detection, where unusual transactions or behaviors in financial data may signify fraudulent activities.\n",
    "\n",
    "6. **Monitoring Equipment Health:**\n",
    "   - In industrial settings, anomaly detection is utilized to monitor the health of equipment. Unusual vibrations, temperature readings, or other deviations from normal operating conditions can signal potential issues.\n",
    "\n",
    "7. **Quality Control in Manufacturing:**\n",
    "   - Anomaly detection is applied to detect defective products on the production line by identifying instances that deviate from standard quality specifications.\n",
    "\n",
    "8. **Health Monitoring:**\n",
    "   - In healthcare, anomaly detection can be used to identify unusual patterns in patient data, potentially indicating health issues or abnormalities.\n",
    "\n",
    "**Common Techniques for Anomaly Detection:**\n",
    "\n",
    "1. **Statistical Methods:**\n",
    "   - Statistical techniques, such as z-scores, Mahalanobis distance, or percentile-based methods, compare the distribution of data points to identify those that deviate significantly.\n",
    "\n",
    "2. **Machine Learning Algorithms:**\n",
    "   - Supervised and unsupervised machine learning algorithms, including isolation forests, one-class SVM, k-nearest neighbors (KNN), and autoencoders, can be trained to distinguish normal from anomalous patterns.\n",
    "\n",
    "3. **Time Series Analysis:**\n",
    "   - Anomaly detection in time series data involves identifying deviations from expected temporal patterns, often using methods like moving averages or seasonality analysis.\n",
    "\n",
    "4. **Clustering Techniques:**\n",
    "   - Clustering algorithms, such as k-means or DBSCAN, can be used to identify clusters of normal data and flag instances that fall outside these clusters.\n",
    "\n",
    "5. **Ensemble Methods:**\n",
    "   - Combining multiple anomaly detection methods using ensemble techniques can enhance the robustness of the detection process.\n",
    "\n",
    "The choice of technique depends on the nature of the data, the characteristics of anomalies, and the specific goals of the application. Anomaly detection is a critical component in various domains, contributing to data quality assurance, security, fraud prevention, and overall system reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebc444-036b-4458-a12b-c49955542809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbbb03e2-58e4-4fd8-97be-69952a08d768",
   "metadata": {},
   "source": [
    "Anomaly detection poses several challenges due to the diverse nature of data and the complexity of identifying deviations from normal patterns. The key challenges in anomaly detection include:\n",
    "\n",
    "1. **Class Imbalance:**\n",
    "   - Anomalies are typically rare events, leading to imbalanced datasets where normal instances significantly outnumber anomalous ones. This imbalance can affect the performance of models and lead to biased results.\n",
    "\n",
    "2. **Scalability:**\n",
    "   - As the size of datasets grows, the computational complexity of anomaly detection methods may become a challenge. Some techniques may struggle to scale efficiently to handle large volumes of data.\n",
    "\n",
    "3. **Adaptability to Data Dynamics:**\n",
    "   - Anomalies may exhibit temporal variations, and the normal behavior of a system can evolve over time. Anomaly detection methods should be adaptable to changing patterns and be able to update their understanding of normality.\n",
    "\n",
    "4. **Unsupervised Learning and Lack of Labeled Anomalies:**\n",
    "   - In many real-world scenarios, labeled data for anomalies may be scarce or unavailable. Unsupervised anomaly detection methods need to identify anomalies without relying on labeled examples, making the task more challenging.\n",
    "\n",
    "5. **Noise and Outliers:**\n",
    "   - Noise in the data or the presence of outliers that are not true anomalies can introduce challenges. Discriminating between true anomalies and outliers or noise is a common difficulty.\n",
    "\n",
    "6. **Feature Engineering:**\n",
    "   - Selecting relevant features and creating effective representations of the data is crucial for anomaly detection. In some cases, the absence of meaningful features or the presence of irrelevant ones can hinder the performance of detection methods.\n",
    "\n",
    "7. **Concept Drift:**\n",
    "   - Anomaly detection models may struggle with concept drift, where the statistical properties of the data change over time. Detecting anomalies in dynamically changing environments requires methods that can adapt to evolving patterns.\n",
    "\n",
    "8. **Interpretable Results:**\n",
    "   - Some anomaly detection models, especially complex machine learning algorithms, might produce results that are difficult to interpret. Understanding why a particular instance is flagged as anomalous is essential for practical applications.\n",
    "\n",
    "9. **Anomaly Definition and Subjectivity:**\n",
    "   - Defining what constitutes an anomaly is often subjective and depends on the context. Anomaly detection methods may need to consider domain-specific criteria and user-defined thresholds, adding complexity to the task.\n",
    "\n",
    "10. **Evaluation Metrics:**\n",
    "    - Determining appropriate evaluation metrics for anomaly detection is challenging. Traditional metrics like accuracy may not be suitable due to class imbalance. Metrics such as precision, recall, F1 score, area under the ROC curve (AUC-ROC), or area under the precision-recall curve (AUC-PR) may be more informative.\n",
    "\n",
    "11. **Attack Detection in Security Applications:**\n",
    "    - In security-related anomaly detection, attackers may deliberately try to manipulate data patterns to evade detection. Adversarial attacks can pose a significant challenge in maintaining the effectiveness of anomaly detection systems.\n",
    "\n",
    "Addressing these challenges requires a combination of domain expertise, careful selection of appropriate techniques, and ongoing monitoring and adaptation of anomaly detection models to changing conditions. Researchers and practitioners continuously work on developing more robust and adaptive approaches to overcome these challenges in diverse application domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8b91c-a32f-4894-a4ff-bc9bf680791c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf70cbfc-69ca-4f5d-9252-0b45976e3a94",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches to identifying anomalies in a dataset. The key difference between these methods lies in the availability of labeled data during the training phase.\n",
    "\n",
    "### Unsupervised Anomaly Detection:\n",
    "\n",
    "**1. Training without Labels:**\n",
    "   - **Data Availability:** Unsupervised anomaly detection methods operate without labeled data during the training phase. The algorithm learns the normal patterns from the majority of the data without explicit information about anomalies.\n",
    "  \n",
    "   - **Approaches:** Unsupervised methods include statistical techniques, clustering algorithms, and autoencoders. Examples are isolation forests, one-class SVM, k-means clustering, and Gaussian mixture models.\n",
    "\n",
    "   - **Challenge:** The main challenge is to distinguish anomalies without having labeled instances for training.\n",
    "\n",
    "**2. Lack of Anomaly Examples:**\n",
    "   - **Pros and Cons:** \n",
    "     - *Pros:* Suitable when labeled anomaly examples are scarce or expensive to obtain.\n",
    "     - *Cons:* May struggle with complex anomalies or situations where anomalies exhibit different patterns.\n",
    "\n",
    "**3. Applicability:**\n",
    "   - **Scenarios:** Unsupervised anomaly detection is often used in scenarios where anomalies are rare and diverse, making it challenging to obtain a representative set of labeled anomalies.\n",
    "\n",
    "**4. Example:**\n",
    "   - **Application:** Anomaly detection in network security, fraud detection in financial transactions, or monitoring industrial equipment for unusual behavior.\n",
    "\n",
    "### Supervised Anomaly Detection:\n",
    "\n",
    "**1. Training with Labeled Anomalies:**\n",
    "   - **Data Availability:** In supervised anomaly detection, the algorithm is trained using labeled data that includes both normal instances and instances labeled as anomalies. The model learns the characteristics of anomalies during training.\n",
    "\n",
    "   - **Approaches:** Supervised methods involve using labeled examples for training classification models, such as support vector machines (SVM), decision trees, or neural networks.\n",
    "\n",
    "   - **Challenge:** Requires a sufficiently large and representative labeled dataset that includes examples of anomalies.\n",
    "\n",
    "**2. Anomaly Definition:**\n",
    "   - **Pros and Cons:**\n",
    "     - *Pros:* Effective when labeled anomaly examples are available, allowing the model to learn specific patterns associated with anomalies.\n",
    "     - *Cons:* Limited to the types of anomalies represented in the labeled training set.\n",
    "\n",
    "**3. Applicability:**\n",
    "   - **Scenarios:** Supervised anomaly detection is suitable when labeled examples of anomalies are accessible, and the types of anomalies are well-defined.\n",
    "\n",
    "**4. Example:**\n",
    "   - **Application:** Fraud detection where historical examples of fraud are available for training the model to recognize similar patterns.\n",
    "\n",
    "### Hybrid Approaches:\n",
    "\n",
    "In some cases, hybrid approaches may be used, combining elements of both unsupervised and supervised methods. For instance, unsupervised methods can be employed to pre-process data and identify potential outliers, which are then labeled and used for supervised training.\n",
    "\n",
    "**Considerations:**\n",
    "- The choice between unsupervised and supervised anomaly detection depends on the availability of labeled data, the nature of anomalies, and the specific requirements of the application.\n",
    "- Unsupervised methods are more flexible but may struggle when anomalies are diverse or complex.\n",
    "- Supervised methods can be effective when labeled anomalies are available but may be limited to the types of anomalies represented in the training set.\n",
    "\n",
    "Overall, the selection of the appropriate approach depends on the characteristics of the dataset, the nature of anomalies, and the resources available for obtaining labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e02cc6-7806-4a95-82ba-0a6f42824cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92097156-1235-4f7f-8af5-8a39d671f429",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be broadly categorized into several main types based on their underlying principles and methodologies. Here are the main categories of anomaly detection algorithms:\n",
    "\n",
    "1. **Statistical Methods:**\n",
    "   - **Description:** Statistical anomaly detection methods assume that normal data follows a certain statistical distribution. Anomalies are identified as data points that deviate significantly from this distribution.\n",
    "   - **Examples:**\n",
    "     - Z-Score (Standard Score): Measures how many standard deviations a data point is from the mean.\n",
    "     - Mahalanobis Distance: Measures the distance of a data point from the centroid, accounting for correlations between variables.\n",
    "     - Percentile-based methods: Identify anomalies based on their position in the data's distribution.\n",
    "\n",
    "2. **Machine Learning-Based Methods:**\n",
    "   - **Description:** Machine learning-based anomaly detection methods involve training models on normal data and identifying anomalies based on deviations from the learned patterns.\n",
    "   - **Examples:**\n",
    "     - Isolation Forests: Construct binary trees to isolate anomalies efficiently.\n",
    "     - One-Class Support Vector Machines (SVM): Train an SVM on normal data, and anomalies are identified as data points distant from the decision boundary.\n",
    "     - Clustering Algorithms (e.g., k-means, DBSCAN): Identify anomalies as points that do not belong to any cluster.\n",
    "\n",
    "3. **Density-Based Methods:**\n",
    "   - **Description:** Density-based anomaly detection methods identify anomalies based on deviations from the expected data density. Anomalies are often located in low-density regions.\n",
    "   - **Examples:**\n",
    "     - DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Clusters dense regions and labels points in low-density regions as anomalies.\n",
    "     - LOF (Local Outlier Factor): Measures the local density of a data point compared to its neighbors.\n",
    "\n",
    "4. **Proximity-Based Methods:**\n",
    "   - **Description:** Proximity-based anomaly detection methods identify anomalies based on the distance or dissimilarity between data points.\n",
    "   - **Examples:**\n",
    "     - Nearest Neighbor-Based Methods: Identify anomalies based on the distance to their nearest neighbors.\n",
    "     - Distance-Based Outliers (e.g., using k-nearest neighbors): Define anomalies as points with high distances to their k-nearest neighbors.\n",
    "\n",
    "5. **Information Theory-Based Methods:**\n",
    "   - **Description:** Information theory-based anomaly detection methods measure the amount of information needed to represent the data. Anomalies are points that require more information to describe.\n",
    "   - **Examples:**\n",
    "     - Kolmogorov Complexity-Based Methods: Identify anomalies based on the complexity of representing a data point.\n",
    "\n",
    "6. **Clustering-Based Methods:**\n",
    "   - **Description:** Clustering-based anomaly detection methods group similar data points into clusters and identify anomalies as points that do not conform to any cluster.\n",
    "   - **Examples:**\n",
    "     - Hierarchical Clustering: Builds a tree of clusters, and anomalies are points not belonging to any cluster.\n",
    "     - Subspace Clustering: Identifies anomalies in specific subspaces of the feature space.\n",
    "\n",
    "7. **Ensemble Methods:**\n",
    "   - **Description:** Ensemble methods combine multiple anomaly detection techniques to improve overall robustness and performance.\n",
    "   - **Examples:**\n",
    "     - Combining statistical methods with machine learning models.\n",
    "     - Stacking multiple anomaly detection models.\n",
    "\n",
    "These categories are not mutually exclusive, and hybrid approaches often combine elements from multiple categories to enhance the effectiveness of anomaly detection systems. The choice of a particular algorithm or method depends on the characteristics of the data, the nature of anomalies, and the goals of the specific application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00388714-b349-4f37-aed5-027d3b5cf212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79415e23-e6a3-44aa-9430-fc9bd1d8bdc8",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods make certain assumptions about the underlying structure of the data, and these assumptions play a key role in how these methods identify anomalies. Here are the main assumptions made by distance-based anomaly detection methods:\n",
    "\n",
    "1. **Normal Data is Clustered:**\n",
    "   - **Assumption:** Distance-based methods assume that normal data points are clustered together in the feature space. This means that most instances in the dataset belong to well-defined groups or clusters, and anomalies are data points located far from these clusters.\n",
    "\n",
    "2. **Anomalies are Isolated:**\n",
    "   - **Assumption:** Anomalies are assumed to be isolated or distant from normal clusters. In other words, they are expected to have larger distances to the nearest neighbors or clusters of normal data points.\n",
    "\n",
    "3. **Euclidean Distance is Meaningful:**\n",
    "   - **Assumption:** Distance-based methods often rely on Euclidean distance or other distance metrics to measure the proximity between data points. The assumption is that the Euclidean distance provides a meaningful measure of dissimilarity in the feature space.\n",
    "\n",
    "4. **Data is Homogeneous:**\n",
    "   - **Assumption:** Distance-based methods assume homogeneity in the dataset, meaning that the majority of data points follow a similar pattern or distribution. Anomalies are considered instances that deviate significantly from this homogeneous pattern.\n",
    "\n",
    "5. **Uniform Density:**\n",
    "   - **Assumption:** The distribution of normal data is assumed to be relatively uniform. In other words, normal data points are evenly distributed within clusters, and anomalies are expected to be located in regions of lower density.\n",
    "\n",
    "6. **Data Points are Independent:**\n",
    "   - **Assumption:** The independence of data points is often assumed. Each data point is treated as an independent entity, and the relationships or dependencies between data points are not explicitly considered in the distance calculations.\n",
    "\n",
    "7. **Constant Density:**\n",
    "   - **Assumption:** Distance-based methods often assume a constant density of normal data points within clusters. Anomalies are identified based on deviations from this assumed constant density.\n",
    "\n",
    "8. **Global Structure Matters:**\n",
    "   - **Assumption:** Distance-based methods typically focus on the global structure of the data. They assume that anomalies can be identified by considering the overall arrangement of data points in the entire feature space rather than local structures.\n",
    "\n",
    "It's important to note that the effectiveness of distance-based anomaly detection methods depends on the degree to which these assumptions hold in a given dataset. If the data violates these assumptions, distance-based methods may be less accurate in identifying anomalies. Additionally, the choice of distance metric and the method for determining distance thresholds or anomaly scores can also impact the performance of these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2672d-91f4-48b8-895a-303ed5c308fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2afd1061-ae57-48ef-8d2e-096af7ae6896",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores based on the local density deviation of data points in comparison to their neighbors. LOF is a density-based anomaly detection algorithm that assesses the local variations in point densities to identify anomalies. Here's an overview of how LOF computes anomaly scores:\n",
    "\n",
    "1. **Local Density Estimation:**\n",
    "   - LOF calculates the local density of each data point by considering the inverse of the average reachability distance to its k-nearest neighbors. The reachability distance measures how easily a point can be reached from its neighbors.\n",
    "   - For each data point \\(p\\), LOF calculates its local density (\\(D(p)\\)) as the inverse of the average reachability distance to its k-nearest neighbors:\n",
    "     \\[ D(p) = \\frac{1}{{\\text{{avg}}(\\text{{reach-dist}}(p, N_k(p)))}} \\]\n",
    "   - \\(N_k(p)\\) represents the set of \\(k\\)-nearest neighbors of point \\(p\\), and \\(\\text{{reach-dist}}(p, q)\\) is the reachability distance from \\(p\\) to \\(q\\).\n",
    "\n",
    "2. **Local Reachability Density:**\n",
    "   - The local reachability density (\\(RD(p, o)\\)) measures the local density of a data point \\(o\\) relative to the local density of \\(p\\). It is the inverse of the average reachability distance from \\(o\\) to its \\(k\\)-nearest neighbors:\n",
    "     \\[ RD(p, o) = \\frac{1}{{\\text{{avg}}(\\text{{reach-dist}}(o, N_k(o)))}} \\]\n",
    "\n",
    "3. **Local Outlier Factor (LOF):**\n",
    "   - The Local Outlier Factor for a data point \\(p\\) is then computed as the average ratio of its local reachability densities to the local densities of its \\(k\\)-nearest neighbors. It reflects how much more or less dense a point is compared to its neighbors:\n",
    "     \\[ LOF(p) = \\frac{{\\text{{avg}}(RD(p, o))}}{{D(p)}} \\]\n",
    "   - A high LOF value indicates that the local density of the point \\(p\\) is lower than that of its neighbors, suggesting that \\(p\\) is potentially an outlier.\n",
    "\n",
    "4. **Anomaly Score:**\n",
    "   - The anomaly score for each data point is the computed LOF value. A higher LOF value indicates a higher likelihood of the point being an outlier or anomaly.\n",
    "\n",
    "5. **Threshold Setting:**\n",
    "   - Anomaly scores can be used to identify outliers by comparing them to a predefined threshold. Points with LOF values above the threshold are considered anomalies.\n",
    "\n",
    "In summary, LOF evaluates the local density of each data point and compares it to the local densities of its neighbors. Points with significantly lower local densities compared to their neighbors are assigned higher LOF values and are considered potential outliers. The strength of LOF lies in its ability to detect anomalies that may not be easily identified using global density estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32736be-acff-4ced-a43b-f53e659c8988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1535a9ed-433f-4326-9f84-772175250dfe",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm, a popular and efficient anomaly detection algorithm, has a few key parameters that influence its behavior. Understanding these parameters is crucial for effectively applying the algorithm to different datasets. The main parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "1. **Number of Trees (\\(n\\_estimators\\)):**\n",
    "   - **Description:** This parameter determines the number of isolation trees to build during the training phase. Increasing the number of trees can lead to a more robust model but may also increase computation time.\n",
    "   - **Default:** Typically set to 100.\n",
    "\n",
    "2. **Maximum Samples (\\(max\\_samples\\)):**\n",
    "   - **Description:** It defines the maximum number of samples to be drawn when creating each isolation tree. A smaller value can lead to more randomness and diversity in the trees, but it may also result in less accurate models.\n",
    "   - **Default:** Often set to \"auto,\" which means \\(max\\_samples\\) is set to the size of the input dataset.\n",
    "\n",
    "3. **Contamination:**\n",
    "   - **Description:** This parameter represents the expected proportion of anomalies in the dataset. It is used to set the decision threshold for classifying instances as anomalies. A higher contamination value increases the sensitivity to anomalies.\n",
    "   - **Default:** Typically set based on domain knowledge or estimated from the dataset.\n",
    "\n",
    "4. **Maximum Features (\\(max\\_features\\)):**\n",
    "   - **Description:** It specifies the maximum number of features to consider when creating each split in the isolation trees. Setting it to a smaller value can increase the diversity of the trees and may be useful for high-dimensional datasets.\n",
    "   - **Default:** Often set to the number of features in the input dataset.\n",
    "\n",
    "5. **Bootstrap:**\n",
    "   - **Description:** This binary parameter indicates whether the training samples should be drawn with or without replacement. Bootstrapping introduces randomness and diversity in the construction of each tree.\n",
    "   - **Default:** Set to True, indicating that bootstrapping is used.\n",
    "\n",
    "6. **Random Seed (\\(random\\_state\\)):**\n",
    "   - **Description:** It allows setting a seed for reproducibility. Providing a specific random seed ensures that the algorithm produces the same results when run with the same input data and parameters.\n",
    "   - **Default:** None, which means the random seed is not set.\n",
    "\n",
    "The values chosen for these parameters can impact the performance of the Isolation Forest algorithm. It's often recommended to experiment with different parameter values, perform cross-validation, and assess the algorithm's performance using appropriate evaluation metrics. Additionally, the choice of parameters may depend on the characteristics of the dataset and the specific requirements of the anomaly detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7663efd-7414-498d-9424-46920ea76e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93c38819-90f8-4595-bab8-e0d003f9c478",
   "metadata": {},
   "source": [
    "The anomaly score for a data point in K-Nearest Neighbors (KNN) is typically computed based on the density of its local neighborhood. One common approach to calculating the anomaly score is to consider the distance to the K-th nearest neighbor. In this case, you mentioned that K is set to 10.\n",
    "\n",
    "The anomaly score can be computed as follows:\n",
    "\n",
    "1. **Calculate the Distance to the K-th Nearest Neighbor (D):**\n",
    "   - For each data point, calculate the distance to its 10th nearest neighbor.\n",
    "\n",
    "2. **Normalize the Distance:**\n",
    "   - Normalize the distance by dividing it by the maximum distance within the dataset. This normalization step is performed to obtain a score between 0 and 1.\n",
    "\n",
    "3. **Anomaly Score:**\n",
    "   - The anomaly score is then obtained by subtracting the normalized distance from 1. The idea is that points with a higher normalized distance (farther from their neighbors) will have a lower anomaly score, and points with a lower normalized distance will have a higher anomaly score.\n",
    "\n",
    "Given that you have mentioned a specific scenario with only 2 neighbors of the same class within a radius of 0.5, it's important to note that KNN may not have 10 neighbors within the specified radius. In your case, the algorithm may consider fewer neighbors if there are not enough points within the radius.\n",
    "\n",
    "Without knowing the exact distances and the dataset, it's challenging to provide a specific numerical value for the anomaly score. The computation would depend on the distances to the neighbors, the normalization, and the specific formula used to derive the anomaly score. If you have the exact distances, you can follow the steps outlined above to calculate the anomaly score for the given data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c302b78-e390-4fa2-b810-5a69fa973ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9b84f-360f-4176-bb87-34af0918b247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
